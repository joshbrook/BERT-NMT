{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy\n\n#!pip install keras-nlp\n#from keras_nlp.layers import PositionEmbedding, TransformerEncoder, TransformerDecoder","metadata":{"execution":{"iopub.status.busy":"2023-03-05T13:38:09.144791Z","iopub.execute_input":"2023-03-05T13:38:09.145499Z","iopub.status.idle":"2023-03-05T13:38:10.136350Z","shell.execute_reply.started":"2023-03-05T13:38:09.145460Z","shell.execute_reply":"2023-03-05T13:38:10.135281Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"small = pd.read_csv(\"/kaggle/input/bert-nmt/pairs.tsv\", sep='\\t', usecols=[1,3], names=['English', 'Gaeilge'])\ndcep = pd.read_csv(\"/kaggle/input/dcep-bisentences/EN-GA-bisentences.txt\", sep='\\t', names=['English', 'Gaeilge'])\n\npairs = pd.concat([small, dcep])\npairs.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T13:15:39.718498Z","iopub.execute_input":"2023-03-05T13:15:39.719801Z","iopub.status.idle":"2023-03-05T13:15:40.064097Z","shell.execute_reply.started":"2023-03-05T13:15:39.719752Z","shell.execute_reply":"2023-03-05T13:15:40.062985Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                 English  \\\n19287  the interpretation and application of the Rule...   \n11127  Admissible petitions shall be considered by th...   \n36059  In addition to the provisions of points 10.1 t...   \n26256  The President may draw up, for the first part ...   \n34185                      INTERNAL BUDGETARY PROCEDURES   \n\n                                                 Gaeilge  \n19287  léirmhíniú agus cur i bhfeidhm na Rialacha Nós...  \n11127  Déanfaidh an coiste freagrach achainíocha ingh...  \n36059  I dteannta fhorálacha phointe 10.1 go pointe 1...  \n26256  Féadfaidh an tUachtarán liosta cainteoirí a th...  \n34185           NÓSANNA IMEACHTA BUISÉADACHA INMHEÁNACHA  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English</th>\n      <th>Gaeilge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19287</th>\n      <td>the interpretation and application of the Rule...</td>\n      <td>léirmhíniú agus cur i bhfeidhm na Rialacha Nós...</td>\n    </tr>\n    <tr>\n      <th>11127</th>\n      <td>Admissible petitions shall be considered by th...</td>\n      <td>Déanfaidh an coiste freagrach achainíocha ingh...</td>\n    </tr>\n    <tr>\n      <th>36059</th>\n      <td>In addition to the provisions of points 10.1 t...</td>\n      <td>I dteannta fhorálacha phointe 10.1 go pointe 1...</td>\n    </tr>\n    <tr>\n      <th>26256</th>\n      <td>The President may draw up, for the first part ...</td>\n      <td>Féadfaidh an tUachtarán liosta cainteoirí a th...</td>\n    </tr>\n    <tr>\n      <th>34185</th>\n      <td>INTERNAL BUDGETARY PROCEDURES</td>\n      <td>NÓSANNA IMEACHTA BUISÉADACHA INMHEÁNACHA</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train = pairs.sample(frac=0.8)\nval = pairs.drop(train.index)\n\nprint(f\"{len(pairs)} total pairs\")\nprint(f\"{len(train)} training pairs\")\nprint(f\"{len(val)} validation pairs\")","metadata":{"execution":{"iopub.status.busy":"2023-03-05T13:15:40.065616Z","iopub.execute_input":"2023-03-05T13:15:40.065968Z","iopub.status.idle":"2023-03-05T13:15:40.312822Z","shell.execute_reply.started":"2023-03-05T13:15:40.065933Z","shell.execute_reply":"2023-03-05T13:15:40.311728Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"48806 total pairs\n39045 training pairs\n9028 validation pairs\n","output_type":"stream"}]},{"cell_type":"code","source":"size = 15000\nseq_len = 20\nbatch = 64\n\nen_vec = layers.TextVectorization(max_tokens=size, output_mode=\"int\", output_sequence_length=seq_len)\nga_vec = layers.TextVectorization(max_tokens=size, output_mode=\"int\", output_sequence_length=seq_len+1)\n\nen_vec.adapt(pairs[\"English\"])\nga_vec.adapt(pairs[\"Gaeilge\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-05T13:15:40.315326Z","iopub.execute_input":"2023-03-05T13:15:40.317284Z","iopub.status.idle":"2023-03-05T13:15:50.759555Z","shell.execute_reply.started":"2023-03-05T13:15:40.317240Z","shell.execute_reply":"2023-03-05T13:15:50.758292Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def format_dataset(en, ga):\n    en = en_vec(en)\n    ga = ga_vec(ga)\n    return ({\"encoder_inputs\": en, \"decoder_inputs\": ga[:, :-1],}, ga[:, 1:])\n\ndef make_dataset(en, ga):\n    dataset = tf.data.Dataset.from_tensor_slices((en, ga))\n    dataset = dataset.batch(batch)\n    dataset = dataset.map(format_dataset)\n    return dataset.shuffle(2048).prefetch(16).cache()\n\nds = make_dataset(train[\"English\"], train[\"Gaeilge\"])\nval_ds = make_dataset(val[\"English\"], val[\"Gaeilge\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-05T13:15:50.760926Z","iopub.execute_input":"2023-03-05T13:15:50.761329Z","iopub.status.idle":"2023-03-05T13:15:50.996389Z","shell.execute_reply.started":"2023-03-05T13:15:50.761289Z","shell.execute_reply":"2023-03-05T13:15:50.995377Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"A chained-together TransformerEncoder and TransformerDecoder makes up our sequence-to-sequence Transformer. A PositionalEmbedding layer is also used to inform the model of word order.\n\nThe TransformerEncoder will receive the original sequence and create a new representation. The TransformerDecoder will then receive this modified representation and the current target sequence (target words 0 to N). The TransformerDecoder will next try to anticipate the following words (up to N+1) in the target sequence.\n\nCausal masking is a crucial component that enables this (see TransformerDecoder function get causal attention mask() for more information). We must ensure that the TransformerDecoder only takes data from target tokens 0 to N when predicting token N+1 because it sees the full sequences at once.","metadata":{}},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n        attention_output = self.attention(\n            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n        )\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(\n            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n        )\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n        )\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n        proj_output = self.dense_proj(out_2)\n        return self.layernorm_3(out_2 + proj_output)\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T13:15:50.998403Z","iopub.execute_input":"2023-03-05T13:15:50.998764Z","iopub.status.idle":"2023-03-05T13:15:51.020232Z","shell.execute_reply.started":"2023-03-05T13:15:50.998726Z","shell.execute_reply":"2023-03-05T13:15:51.019087Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"embed_dim = 256\nlatent_dim = 2048\nnum_heads = 64\n\ndef create_model(embed_dim, latent_dim, num_heads):\n    encoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n    pos = PositionalEmbedding(seq_len, size, embed_dim)(encoder_inputs)\n    encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(pos)\n    encoder = Model(encoder_inputs, encoder_outputs)\n\n    decoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n    encoded_seq_inputs = layers.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n    x = PositionalEmbedding(seq_len, size, embed_dim)(decoder_inputs)\n    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n    x = layers.Dropout(0.5)(x)\n    decoder_outputs = layers.Dense(size, activation=\"softmax\")(x)\n    decoder = Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n\n    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n    transformer = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n    \n    return transformer","metadata":{"execution":{"iopub.status.busy":"2023-03-05T13:17:32.489790Z","iopub.execute_input":"2023-03-05T13:17:32.490524Z","iopub.status.idle":"2023-03-05T13:17:32.499082Z","shell.execute_reply.started":"2023-03-05T13:17:32.490486Z","shell.execute_reply":"2023-03-05T13:17:32.498008Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"epochs = 30  # This should be at least 30 for convergence\n\ntransformer = create_model(embed_dim, latent_dim, num_heads)\ntransformer.summary()\ntransformer.compile(\n    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)\ntransformer.fit(ds, epochs=epochs, validation_data=val_ds)","metadata":{"execution":{"iopub.status.busy":"2023-03-04T15:13:52.824618Z","iopub.execute_input":"2023-03-04T15:13:52.825023Z","iopub.status.idle":"2023-03-04T16:15:44.248994Z","shell.execute_reply.started":"2023-03-04T15:13:52.824995Z","shell.execute_reply":"2023-03-04T16:15:44.247795Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"transformer\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n                                                                                                  \n positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n alEmbedding)                                                                                     \n                                                                                                  \n decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n                                                                                                  \n transformer_encoder (Transform  (None, None, 256)   17878528    ['positional_embedding[0][0]']   \n erEncoder)                                                                                       \n                                                                                                  \n model_1 (Functional)           (None, None, 15000)  42405784    ['decoder_inputs[0][0]',         \n                                                                  'transformer_encoder[0][0]']    \n                                                                                                  \n==================================================================================================\nTotal params: 64,129,432\nTrainable params: 64,129,432\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/30\n611/611 [==============================] - 136s 205ms/step - loss: 4.6578 - accuracy: 0.2419 - val_loss: 3.2496 - val_accuracy: 0.3863\nEpoch 2/30\n611/611 [==============================] - 109s 179ms/step - loss: 2.8961 - accuracy: 0.4399 - val_loss: 2.0746 - val_accuracy: 0.5671\nEpoch 3/30\n611/611 [==============================] - 109s 179ms/step - loss: 1.9382 - accuracy: 0.5967 - val_loss: 1.4482 - val_accuracy: 0.7132\nEpoch 4/30\n611/611 [==============================] - 109s 179ms/step - loss: 1.2566 - accuracy: 0.7343 - val_loss: 0.7961 - val_accuracy: 0.8426\nEpoch 5/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.8003 - accuracy: 0.8354 - val_loss: 0.5258 - val_accuracy: 0.9091\nEpoch 6/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.5876 - accuracy: 0.8812 - val_loss: 0.4191 - val_accuracy: 0.9333\nEpoch 7/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.4617 - accuracy: 0.9071 - val_loss: 0.4181 - val_accuracy: 0.9323\nEpoch 8/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.3708 - accuracy: 0.9260 - val_loss: 0.3318 - val_accuracy: 0.9536\nEpoch 9/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.3061 - accuracy: 0.9388 - val_loss: 0.3129 - val_accuracy: 0.9583\nEpoch 10/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.2614 - accuracy: 0.9480 - val_loss: 0.2973 - val_accuracy: 0.9639\nEpoch 11/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.2275 - accuracy: 0.9545 - val_loss: 0.2879 - val_accuracy: 0.9656\nEpoch 12/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.1988 - accuracy: 0.9602 - val_loss: 0.2908 - val_accuracy: 0.9661\nEpoch 13/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.1787 - accuracy: 0.9639 - val_loss: 0.2786 - val_accuracy: 0.9695\nEpoch 14/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.1578 - accuracy: 0.9680 - val_loss: 0.2761 - val_accuracy: 0.9701\nEpoch 15/30\n611/611 [==============================] - 110s 179ms/step - loss: 0.1453 - accuracy: 0.9703 - val_loss: 0.2741 - val_accuracy: 0.9710\nEpoch 16/30\n611/611 [==============================] - 110s 180ms/step - loss: 0.1295 - accuracy: 0.9736 - val_loss: 0.2759 - val_accuracy: 0.9719\nEpoch 17/30\n611/611 [==============================] - 110s 179ms/step - loss: 0.1175 - accuracy: 0.9761 - val_loss: 0.2774 - val_accuracy: 0.9717\nEpoch 18/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.1089 - accuracy: 0.9780 - val_loss: 0.2814 - val_accuracy: 0.9728\nEpoch 19/30\n611/611 [==============================] - 110s 179ms/step - loss: 0.1004 - accuracy: 0.9795 - val_loss: 0.2755 - val_accuracy: 0.9727\nEpoch 20/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.0947 - accuracy: 0.9807 - val_loss: 0.2768 - val_accuracy: 0.9727\nEpoch 21/30\n611/611 [==============================] - 110s 179ms/step - loss: 0.0883 - accuracy: 0.9822 - val_loss: 0.2818 - val_accuracy: 0.9728\nEpoch 22/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.0832 - accuracy: 0.9831 - val_loss: 0.2828 - val_accuracy: 0.9723\nEpoch 23/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.0773 - accuracy: 0.9844 - val_loss: 0.2862 - val_accuracy: 0.9729\nEpoch 24/30\n611/611 [==============================] - 119s 195ms/step - loss: 0.0738 - accuracy: 0.9849 - val_loss: 0.2894 - val_accuracy: 0.9723\nEpoch 25/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.0703 - accuracy: 0.9857 - val_loss: 0.2845 - val_accuracy: 0.9735\nEpoch 26/30\n611/611 [==============================] - 110s 180ms/step - loss: 0.0666 - accuracy: 0.9864 - val_loss: 0.2888 - val_accuracy: 0.9742\nEpoch 27/30\n611/611 [==============================] - 110s 180ms/step - loss: 0.0632 - accuracy: 0.9872 - val_loss: 0.2817 - val_accuracy: 0.9736\nEpoch 28/30\n611/611 [==============================] - 110s 180ms/step - loss: 0.0612 - accuracy: 0.9875 - val_loss: 0.2901 - val_accuracy: 0.9734\nEpoch 29/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.0588 - accuracy: 0.9880 - val_loss: 0.2932 - val_accuracy: 0.9738\nEpoch 30/30\n611/611 [==============================] - 109s 179ms/step - loss: 0.0567 - accuracy: 0.9883 - val_loss: 0.2927 - val_accuracy: 0.9739\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f8493807410>"},"metadata":{}}]},{"cell_type":"code","source":"ga_vocab = ga_vec.get_vocabulary()\nga_index_lookup = dict(zip(range(len(ga_vocab)), ga_vocab))\nmax_decoded_sentence_length = 20\n\n\ndef decode_sequence(model, input_sentence):\n    tokenized_input_sentence = en_vec([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = ga_vec([decoded_sentence])[:, :-1]\n        predictions = model([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = ga_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\n\nfor _ in range(5):\n    try:\n        inp = \" \".join(nltk.word_tokenize(random.choice(pairs[\"English\"])))\n        translated = decode_sequence(new_model, inp)\n        print(inp, \"\\n\", \"--> \", translated[8:], \"\\n\")\n    except:\n        continue\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-05T14:37:43.115964Z","iopub.execute_input":"2023-03-05T14:37:43.117028Z","iopub.status.idle":"2023-03-05T14:37:48.377828Z","shell.execute_reply.started":"2023-03-05T14:37:43.116969Z","shell.execute_reply":"2023-03-05T14:37:48.376687Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"The Ombudsman may advise the complainant to apply to another authority . \n -->  an tombudsman a rá leis an ngearánach a chur i gcomhréir leis an údarás eile      \n\nConsultation of sensitive documents by the members of the Special Committee of the European Parliament shall take place in a secured room at the Council premises . \n -->  féachaint ar dhoiciméid íogaire a fháil ón uachtarán pharlaimint na heorpa agus gur cheart fiosrúcháin a ghlacadh sa bhliain ar \n\nSimplified procedure \n -->  procedure                    \n\nThe Chairs and rapporteurs of the committee responsible and of any associated committees shall jointly take appropriate action to ensure that Parliament is provided with immediate , regular and full information , if necessary on a confidential basis , at all stages of the negotiation and conclusion of international agreements , including the draft and the finally adopted text of negotiating directives , and with the information referred to in paragraph 3 , \n -->  cathaoirligh agus rapóirtéirí an choiste fhreagraigh agus na gcoistí comhlachaithe i gcás ina comhpháirteach chun a áirithiú go soláthraíonn an \n\nThe establishment and operation of the register shall respect the rights of Members of the European Parliament to exercise their parliamentary mandate without restriction , and shall not impede access for Members ' constituents to the European Parliament 's premises . \n -->  parlaimint na heorpa agus an chomhbheartais eachtraigh agus an chláir sin i dtaobh saincheisteanna slándála     más \n\n","output_type":"stream"}]},{"cell_type":"code","source":"transformer.save_weights(\"weights\")","metadata":{"execution":{"iopub.status.busy":"2023-03-04T16:39:11.966161Z","iopub.execute_input":"2023-03-04T16:39:11.967105Z","iopub.status.idle":"2023-03-04T16:39:13.012855Z","shell.execute_reply.started":"2023-03-04T16:39:11.967066Z","shell.execute_reply":"2023-03-04T16:39:13.011398Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"new_model = create_model(embed_dim, latent_dim, num_heads)\nnew_model.load_weights(\"/kaggle/input/weights/weights\")\nnew_model","metadata":{"execution":{"iopub.status.busy":"2023-03-05T13:27:21.239775Z","iopub.execute_input":"2023-03-05T13:27:21.240182Z","iopub.status.idle":"2023-03-05T13:27:24.202950Z","shell.execute_reply.started":"2023-03-05T13:27:21.240143Z","shell.execute_reply":"2023-03-05T13:27:24.201896Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<keras.engine.functional.Functional at 0x7f84942b07d0>"},"metadata":{}}]},{"cell_type":"code","source":"inp = \"The parliament decided on the proposal\"\n\ntranslated = decode_sequence(new_model, inp)\nprint(inp, \"\\n\", translated, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-03-05T14:37:25.421062Z","iopub.execute_input":"2023-03-05T14:37:25.421981Z","iopub.status.idle":"2023-03-05T14:37:26.434368Z","shell.execute_reply.started":"2023-03-05T14:37:25.421929Z","shell.execute_reply":"2023-03-05T14:37:26.433201Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"The parliament decided on the proposal \n [start] an pharlaimint cinneadh ar an togra               \n\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\nsample = val.sample(5)\nbleu = []\nfor en, ga in zip(sample[\"English\"], sample[\"Gaeilge\"]):\n    en_tk = \" \".join(nltk.word_tokenize(en))\n    ga_tk = nltk.word_tokenize(ga.lower())\n    \n    smoothie = SmoothingFunction().method7\n    translation = decode_sequence(new_model, en_tk)[8:]\n    score = sentence_bleu([ga_tk], translation.split(), smoothing_function=smoothie)\n    \n    print(en_tk, \"\\n\", \" \".join(ga_tk), \"\\n\", translation, \"\\n\", score, \"\\n\")\n    bleu.append(score)\n\nnp.mean(bleu)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T14:35:13.183962Z","iopub.execute_input":"2023-03-05T14:35:13.184654Z","iopub.status.idle":"2023-03-05T14:35:18.180109Z","shell.execute_reply.started":"2023-03-05T14:35:13.184617Z","shell.execute_reply":"2023-03-05T14:35:18.178990Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"Ordinary Treaty revision \n gnáth-athbhreithniú ar na conarthaí \n an tombudsman                   \n 0 \n\nThe President shall notify the committee of that initiative and inform Parliament . \n tabharfaidh an tuachtarán fógra faoin tionscnamh sin don choiste agus cuirfidh sé an pharlaimint ar an eolas faoi . \n an tuachtarán agus cuirfidh sé an choiste agus an méid sin in iúl don pharlaimint a chur ar an eolas \n 0.3130668681014416 \n\nCommittee on Industry , Research and Energy \n an coiste um thionsclaíocht , um thaighde agus um fhuinneamh \n coiste um an um thaighde agus um fhuinneamh             \n 0.5428797663312449 \n\nRequests for the application of Rule 50 of the Rules of Procedure shall be submitted no later than the Monday preceding the meeting of the Conference of Committee Chairs at which requests to draw up own-initiative reports are to be dealt with . \n déanfar iarrataí ar chur i bhfeidhm riail 50 de na rialacha nós imeachta a thíolacadh tráth nach déanaí ná an luan roimh an gcruinniú de chomhdháil chathaoirligh na gcoistí ar lena linn a dhéileálfar le hiarrataí chun tuarascálacha féintionscnaimh a tharraingt suas . \n iarrataí ar chur i bhfeidhm riail 50 de na rialacha nós imeachta a thíolacadh tráth nach déanaí ná an luan \n 0.3536031928232399 \n\nreferral back \n tarchur ar ais \n of ais                   \n 0.14702339453511146 \n\n","output_type":"stream"},{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"0.27131464435820757"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}